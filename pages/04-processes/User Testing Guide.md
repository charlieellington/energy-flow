
## ğŸ§ª How to Run Effective User Tests 
This approach is fast, reliable, and replaces a days of manual set up and analysis with AI.


### Context 

We used to do user testing by the process out of Google Venturesâ€™ Design Sprint. At my last business, a design studio called Deep Work, we kept the heartâ€”**align, prototype, observe**â€”and stripped down to the essentials whilst improving the process after each client, aimed at early stage startups. 

1. **Align.** Gather the team, agree on one sharp question to answer.
2. **Prototype.** Build the quickest workable versionâ€”a clickable app, a paper sketch, or a staged space.
3. **Observe.** Test one-on-one, watch the user work, listen for friction.

Most sessions happen over Zoom with a digital interface on-screen, but the framework is elastic. Swap the screen for a counter and the bakery itself becomes the prototype: walk, talk, and note what the customer does next. Same rhythm, same insightsâ€”just a different stage.

I created this guide as a short overview for how to reun

### Step 1: Define Your Hypotheses

Start with 1â€“3 clear hypotheses about what you expect or want to learn. For example:

- â€œUsers donâ€™t notice the call-to-action.â€
    
- â€œPeople understand the pricing model.â€
    
- â€œThe onboarding flow is too long.â€
    

Good hypotheses are focused, testable, and tied to user behaviourâ€”not your opinion.

---

### Step 2: Find the Right Testers

Use your customer base or network to recruit 5â€“6 users who match your ideal user profile.

> Research shows this number reveals about 85% of usability issues without becoming data-heavy.

Keep it lightweight: one-on-one is best. No need for big panels or endless scheduling.

---

### Step 3: Write a Script

Your goal isnâ€™t to lead usersâ€”itâ€™s to observe them.

Create a loose script with:

- A short intro (who you are, why theyâ€™re here)
    
- Tasks to complete (e.g. â€œFind a plan youâ€™d buy.â€)
    
- Open-ended prompts (e.g. â€œWhat would you do next?â€)
    

Avoid yes/no questions. You're listening for _confusion_, _hesitation_, and _unexpected paths_.

---

### Step 4: Run the Tests

Do each test live over Zoom or in-person.  
While the user talks:

- Take simple notes (what they do, where they pause, what they say).
    
- **Always record the session**. AI tools like [Otter.ai](https://otter.ai/) can transcribe automatically.
    

Itâ€™s less about what they say theyâ€™ll doâ€”watch what they actually do.

---

### Step 5: AI-Generated Report

After all sessions:

- Drop your raw notes and original hypotheses into ChatGPT.
    
- Prompt it:  
    **â€œHere are 5 user testing transcripts and 3 hypotheses I wanted to test. Write a summary report with findings, patterns, and suggested actions.â€**
    

This removes subjective bias and turns a day of analysis into 15 minutes.

---

### Step 6: Go Deeper with AI

Use follow-up prompts in ChatGPT like:

- â€œWhich hypothesis was most strongly validated?â€
    
- â€œWhat are recurring usability issues?â€
    
- â€œSummarise what confused users and how we might fix it.â€
    

Itâ€™s a conversation. Think of GPT as your co-pilot, not a report generator.

---

### Step 7: Decide on Actions

Review the report with your team. You can do this as a quick async review or in a short workshop.  
Pick 1â€“3 changes youâ€™ll make based on what you learned. Then test again.

---

### Why This Works

This process blends what we did at Deep Work and Google Ventures sprints with the speed of AI.

ğŸ“Œ Less subjective  
ğŸ“Œ Faster turnaround  
ğŸ“Œ Action-oriented insights  
ğŸ“Œ Works well with lean teams

---

Let me know if you want a ready-made prompt template or a Notion workspace for this. Alsoâ€”how might this evolve further using your current projects or workflows? Curious to hear how you'd apply it in Paid247, IFS coaching, or Sunni.be.